<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 on Hov&#39;s Blog</title>
    <link>https://HauyuChen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on Hov&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017. All rights reserved.</copyright>
    <lastBuildDate>Tue, 11 Jul 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://HauyuChen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>【机器学习笔记】0. 什么是机器学习</title>
      <link>https://HauyuChen.github.io/post/ml-0-machinelearning-introduction/</link>
      <pubDate>Tue, 11 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://HauyuChen.github.io/post/ml-0-machinelearning-introduction/</guid>
      <description>作者注：机器学习系列是本人在学习机器学习相关内容时产生的笔记，希望也能对您有所帮助。值得注意的是，作者作为初学者，表述难免有误或不全面，望多批评指正。 如有任何问题，欢迎您随时与我联系：Hauyu.Chen@Gmail.com
版权声明：本文由 Hov 所有，发布于 http://chenhy.com ，转载请注明出处。

0 前言 AI 时代，作为计算机专业的学生，不了解点 AI 相关的知识似乎有点说不过去，尤其是机器学习。 我们一直在说机器学习，但是我们怎么理解机器学习？机器学习解决了什么问题？我们如何将机器学习应用到现实生活中的问题呢？ 本文是机器学习系列的开篇，给出了机器学习的经典定义，并介绍监督学习和无监督学习这两个重要的分支，后续将逐步深入。

1 机器学习的定义 Tom Mitchell 给出了一个关于机器学习的定义，这也是一个被经常引用的定义。
 Tom Mitchell:&amp;ldquo;A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.&amp;rdquo;
 这段话翻译过来就是：对于某类任务T和性能度量P，如果一个计算机程序在T上以P衡量的性能随着经验E而自我完善，那么我们称这个计算机程序在从经验E学习。
举个栗子：AlphaGo下棋
 经验 E = AlphaGo从很多盘棋局获得的经验（学棋）； 任务 T = AlphaGo和对手下棋（下棋）； 性能 P = AlphaGo赢的可能性（赢棋）。</description>
    </item>
    
    <item>
      <title>【机器学习笔记】1. 线性回归（单变量）</title>
      <link>https://HauyuChen.github.io/post/ml-1-linearregression-onevariables/</link>
      <pubDate>Fri, 21 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://HauyuChen.github.io/post/ml-1-linearregression-onevariables/</guid>
      <description>作者注：机器学习系列是本人在学习机器学习相关内容时产生的笔记，希望也能对您有所帮助。值得注意的是，作者作为初学者，表述难免有误或不全面，望多批评指正。 如有任何问题，欢迎您随时与我联系：Hauyu.Chen@Gmail.com
版权声明：本文由 Hov 所有，发布于 http://chenhy.com ，转载请注明出处。

0 前言 上一篇提到了监督学习的概念，本文要讲的线性回归就属于监督学习。 本文通过单变量线性回归讲述线性回归思想。值得注意的是，特征x并非只能是一个，也可以有x1、x2、x3···，这就是多特征的问题了（后续会提到）。事实上，单特征和多特征的思想是一样的，本文讨论单特征（只有一个特征x）的情形。

1 要点 1.1 单变量线性回归模型 上述公式（线性函数）为一个最简单的线性回归模型，hθ(x)为我们的假设函数，x是训练集中给的数据，θ0、θ1为未知参数，我们需要计算出合适的θ0、θ1的值。
1.2 Cost Function（代价函数） 上述公式为代价函数的定义，hθ(x)是预测值，x(i)是训练集中第i组数据中的特征，y(i)是训练集中第i组数据中的结果，J(θ0,θ1)表示的是预测值与实际值的误差（方差），误差当然越小越好，所以我们的目标就是最小化Cost Function，即找出合适的θ0、 θ1使得J(θ0,θ1)最小，这样说明数据拟合得最好。

2 思路 我们来引入一个场景，我们想实现房价的预测。
 房价取决于多方面的因素：面积、地段、楼层等等。为方便讨论，我们先不考虑多变量的情况，只考虑单变量。也就是在地段、楼层等因素一致的情况下，面积(x)对房价(y)的影响。 房价预测问题其实就是找出面积x和房价y的关系 hθ(x) = θ0 + θ1*x，即根据面积(x)去预测房价(y)。 所以问题的核心是找出合适的θ0、θ1，使得我们的预测 hθ(x) = θ0 + θ1*x 是合理的。 衡量 θ0、θ1 是否合适的标准就是代价函数J(θ0,θ1)，θ0、θ1应使得J(θ0,θ1)尽可能小。  假定我们已经有了一个训练集，里面包含面积x和对应的房价y。以横轴表示面积x，以竖轴表示房价y，根据训练集可绘制图形如下：
注：图示并非真实数据，只作参考。
线性回归要做的就是通过训练，找出面积x与房价y之间对应的关系（线性函数），通过训练，我们可以得出一条表示 hθ(x) 的直线，这就是我们的预测。

3 结语 现在我们应该知道线性回归的思想了，就是通过训练集去计算出假设函数，通过假设函数可以实现对结果的预测。 假设函数最关键的就是找出未知参数 θ0、θ1 ，因为这两个未知参数决定我们预测是否准确。 未知参数 θ0、θ1 的选择通过代价函数 J(θ0,θ1) 来评定，我们要让 J(θ0,θ1) 尽可能小。 那我们怎么计算 θ0、θ1 的值呢？ 下篇将引入“梯度下降”的方法，通过此方法可更快地计算出 θ0、θ1 的值。</description>
    </item>
    
    <item>
      <title>【机器学习笔记】2. 梯度下降法</title>
      <link>https://HauyuChen.github.io/post/ml-2-gradientdescent/</link>
      <pubDate>Wed, 26 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://HauyuChen.github.io/post/ml-2-gradientdescent/</guid>
      <description>作者注：机器学习系列是本人在学习机器学习相关内容时产生的笔记，希望也能对您有所帮助。值得注意的是，作者作为初学者，表述难免有误或不全面，望多批评指正。 如有任何问题，欢迎您随时与我联系：Hauyu.Chen@Gmail.com
版权声明：本文由 Hov 所有，发布于 http://chenhy.com ，转载请注明出处。

0 前言 上一篇讲到了线性回归，提到了代价函数（Cost Function）的概念，我们知道我们的目标是找到合适的 θ0、θ1 使得代价函数 J(θ0,θ1) 最小。 但是，若漫无目的地设定 θ0、θ1 的值， J(θ0,θ1) 可能会有无数的结果。 那我们要怎么更快地找到 J(θ0,θ1) 的最小值呢？ 本文将介绍一种重要的优化算法，Gradient Descent（梯度下降法）。

1 什么是梯度？ 在讲解梯度下降法之前，我们必须先了解梯度的概念。 梯度是高等数学中的概念，梯度的指向即为函数增长最快的方向。同理，梯度的反方向即为函数下降最快的方向。 现在你知道为什么梯度下降法是优化算法了吧？它能使我们的代价函数下降的最快！

2 原理 下图为代价函数的三维图形，分别以 θ0、θ1 为 X、Y 轴，以 J(θ0,θ1) 为 Z 轴。求解代价函数最小值的过程可看作是寻找“一座座山坡”中的最低点，因为在“山底”时 J(θ0,θ1)最小。
假定我们随机站在某个山坡上，每次往某个方向向下走一步，怎么走才能最快到山底？这也就是梯度下降法所要解决的，沿着梯度方向最小化 J(θ0,θ1) 。
 确定向下一步的步伐大小，称之为 Learning Rate ； 任取 θ0,θ1 （随机站在某个山坡）； 沿着梯度的反方向，走一个步伐大小，更新 θ0、θ1 ，此时 J(θ0,θ1) 变小； 重复步骤3，当下降的高度小于某个定义的值（已经到山底），则停止下降。  
3 算法  优化目标：J(θ0,θ1)
 优化参数：θ0、θ1</description>
    </item>
    
    <item>
      <title>【机器学习笔记】3. 线性回归（多变量）</title>
      <link>https://HauyuChen.github.io/post/ml-3-linearregression-multiplevariables/</link>
      <pubDate>Mon, 31 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://HauyuChen.github.io/post/ml-3-linearregression-multiplevariables/</guid>
      <description>作者注：机器学习系列是本人在学习机器学习相关内容时产生的笔记，希望也能对您有所帮助。值得注意的是，作者作为初学者，表述难免有误或不全面，望多批评指正。 如有任何问题，欢迎您随时与我联系：Hauyu.Chen@Gmail.com
版权声明：本文由 Hov 所有，发布于 http://chenhy.com ，转载请注明出处。

0 前言 前面我们已经提到了单变量线性回归，现在讲讲多元线性回归（多变量线性回归）。
在讲解单变量线性回归时，我们引入了房价预测这个栗子，仅通过房屋面积来预测房价。事实上，影响房价的因素有很多，如面积、房间数量、楼层、房龄等等。
现在，我们想在进行房价预测时考虑面积、房间数量、楼层、房龄这几个因素，而不是单单只考虑面积。
显然，单变量线性回归已不再适用，我们可以通过多元线性回归来解决。

1 要点 1.1 多元线性回归模型 这条公式是否很熟悉呢？其实就是在单变量线性回归模型的基础上增加了其它的特征 x2、x3、x4 ··· xn。
为方便计算，我们可以定义 x0=1 ，现在可以将公式转化成如下形式：
令： 特征向量X = [x0,x1,x2, &amp;hellip; ,xn] 参数向量θ = [θ0,θ1,θ2, &amp;hellip; ,θn] X 和 θ 均为 n+1 维向量，有：
最后，假设函数h可简化成以下形式：
1.2 Cost Function（代价函数） 注：多元线性回归的代价函数和单变量线性回归的一致，不过增加了一些新的参数θ3、θ4、θ5···θn。
1.3 Batch Gradient Descent（批量梯度下降） 
2 思路 多元线性回归的思路和单变量线性回归大体一致，只不过我们需要对计算公式做一些微小改变。
同样，还是通过房价预测的栗子来讲解：
 房价取决于多方面的因素，在这里我们考虑面积、房间数量、楼层、房龄这四个因素。所以，我们要搞清楚的就是面积 x1 、房间数量 x2 、楼层 x3 、房龄 x4 对房价 y 的影响。 所以，假设函数为 hθ(x) = θ0 + θ1*x1+ θ2*x2 + θ3*x3+ θ4*x4 。 令 x0=1 ，将假设函数转换成 hθ(x) = θ0*x0 + θ1*x1+ θ2*x2 + θ3*x3+ θ4*x4 。 令特征向量 X=[x0,x1,x2,x3,x4] ,参数向量 θ=[θ0,θ1,θ2,θ3,θ4] ,最终我们的假设函数为 hθ(x)=(θ^T)X 。 问题的核心还是找出合适的参数向量θ，使得我们的预测 hθ(x) 是合理的。 衡量参数向量 θ 是否合适的标准就是代价函数 J(0) ，θ 应使得 J(θ) 尽可能小。</description>
    </item>
    
    <item>
      <title>【机器学习笔记】4. 逻辑回归</title>
      <link>https://HauyuChen.github.io/post/ml-4-logisticregression/</link>
      <pubDate>Thu, 03 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://HauyuChen.github.io/post/ml-4-logisticregression/</guid>
      <description>作者注：机器学习系列是本人在学习机器学习相关内容时产生的笔记，希望也能对您有所帮助。值得注意的是，作者作为初学者，表述难免有误或不全面，望多批评指正。 如有任何问题，欢迎您随时与我联系：Hauyu.Chen@Gmail.com
版权声明：本文由 Hov 所有，发布于 http://chenhy.com ，转载请注明出处。

0 前言 本文要讲的逻辑回归属于分类算法，它是对线性回归的改进。
在处理二分类问题的时候，我们可将所有预测 y 映射成某个值。假设，若该值大于等于0.5，则结果为1；若该值小于0.5，则结果为0。这样，我们就将所有样本分为两类了。
若用线性回归去处理分类问题， y 的值可能远大于1或小于0，这样会造成较大的误差，所以能否让 y 的值处于0到1之间呢？
逻辑回归实现的就是这样的功能，将预测值映射到某个固定的区间，通过决策边界，实现二分类问题。

1 要点 1.1 逻辑函数（Sigmoid函数） 逻辑回归中的逻辑函数其实就是线性回归中的假设函数，只不过在假设函数的基础上进行一个函数映射。
（1）线性回归中的假设函数
（2）逻辑回归中的逻辑函数
逻辑函数对应的图像如下：
可见，逻辑函数将所有预测映射到（0，1）区间。
1.2 Decision Boundary（决策边界） 为了对输出结果进行0和1的分类，我们假设认为 hθ(x) 大于等于0.5，则结果 y=1 ；若 hθ(x) 小于0.5，则结果 y=0 ，即：
根据逻辑函数的图像，有：
即：
所以，决策边界就是将结果分为 y=0 和 y=1 的分界，不同的参数向量 θ ，可对应不同的决策边界。
举个栗子：
这个栗子中，决策边界为 x=5 ，因为在其左边，y=1；在其右边，y=0。
当然，决策边界并非只能是直线，也可能是复杂的曲线。
1.3 Cost Function（代价函数） 上述公式可简化如下：
向量化如下：
1.4 Gradient Descent（梯度下降） 
2 思路 假设，我们要实现邮件的分类（垃圾邮件、非垃圾邮件）
 借助线性回归的思路，我们可以设定假设函数 hθ(x)=θ0*x0+θ1*x1+⋯+θn*xn ，但 hθ(x) 的结果可能在 (−∞,+∞) 之间。 所以，我们通过构造逻辑回归将预测值映射到（0，1）区间，预测值表示该邮件是垃圾邮件的概率。 构造代价函数。 通过梯度下降来最优化特征向量 θ ，求得决策边界。</description>
    </item>
    
    <item>
      <title>【机器学习笔记】5. 正则化</title>
      <link>https://HauyuChen.github.io/post/ml-5-regulation/</link>
      <pubDate>Thu, 10 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://HauyuChen.github.io/post/ml-5-regulation/</guid>
      <description>作者注：机器学习系列是本人在学习机器学习相关内容时产生的笔记，希望也能对您有所帮助。值得注意的是，作者作为初学者，表述难免有误或不全面，望多批评指正。 如有任何问题，欢迎您随时与我联系：Hauyu.Chen@Gmail.com
版权声明：本文由 Hov 所有，发布于 http://chenhy.com ，转载请注明出处。

0 前言 前面我们已经提到了线性回归、 逻辑回归的概念。我们通过假设函数去拟合训练集，事实上，在拟合的过程中可能会出现 Overfitting （过拟合）的情况，本文要讲的 Regulation （正则化）是解决 Overfitting 的主要方法之一。

1 拟合问题 在讲解正则化之前，我们需要先了解一下拟合的概念。
还记得房价预测的栗子吗？我们通过房屋面积预测房价，给定一个训练集，我们要用假设函数去拟合这些数据。但其实房屋面积和房价的关系并非线性关系，不能简单地用一条直线来表示。
（1）Underfitting（欠拟合）
下图所示，蓝色直线（假设函数）并不能很好地拟合样本数据，我们将这种情况称为 Underfitting （欠拟合），也可以称为 High bias （高偏差）。
（2）Just Right（刚好拟合）
在原有的假设函数上增加一个二次项，原来的线性函数变成了二次函数，下图为拟合效果，显然，比之前的拟合得更好。蓝色的直线较好地拟合了样本数据，我们可以认为其刚好拟合。
（3）Overfitting（过拟合）
从前两种情况，我们发现增加了新的参数（增加了一个二次项），数据拟合得更好了。现在，我们增加一个三次项和四次项，拟合效果将如下所示，蓝色的线经过了每一个点，而且上下扭曲。这就属于 Overfitting （过拟合），也可称之为 High Variance （高方差）。
你也许会问，我们不是说要让假设函数尽量拟合吗？为什么现在每个数据都拟合了，我们还要去处理它呢？
这是因为，尽管现在的数据拟合得很完美，但它只是和训练样本的拟合，如果我们加入新的测试样本，这样的假设函数往往预测的效果很糟糕。
也就是说，这种模型无法泛化到新的数据，无法预测新的数据。（泛化：指一个假设模型应用到新样本的能力）
同理，在逻辑回归中也存在欠拟合和过拟合的情况。
（1）逻辑回归中的欠拟合
（2）逻辑回归中的刚好拟合
（3）逻辑回归中的过拟合

2 Regulation（正则化） 前面我们提到，我们应该避免出现过拟合的情况，因为这样会使得我们的预测不准确。解决过拟合问题有两种主要的方法：一种是通过减少特征数量；另一种是正则化。
正则化可以保持特征数量不变，通过设置参数的权重来解决过拟合问题。
2.1 正则化线性回归 2.1.1 Cost Function（代价函数） 上面的公式为正则化线性回归的代价函数，蓝色方块为一般线性回归的代价函数，红色方块为正则化项，而 λ 为正则化参数。
正则化参数 λ 需要我们自行选择一个合适的数值，如 λ=1 。
2.1.2 Gradient Descent（梯度下降） 注： θ0 应单独计算，因为不需要惩罚参数 θ0 ，惩罚参数从 θ1 开始。即为正则化操作。</description>
    </item>
    
  </channel>
</rss>