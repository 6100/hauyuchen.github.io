<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>线性回归 on Hov&#39;s Blog</title>
    <link>https://HauyuChen.github.io/topics/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</link>
    <description>Recent content in 线性回归 on Hov&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017. All rights reserved.</copyright>
    <lastBuildDate>Fri, 21 Jul 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://HauyuChen.github.io/topics/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>【机器学习笔记】1. 线性回归（单变量）</title>
      <link>https://HauyuChen.github.io/post/linearregression-onevariables/</link>
      <pubDate>Fri, 21 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://HauyuChen.github.io/post/linearregression-onevariables/</guid>
      <description>作者注：机器学习系列是本人在学习机器学习相关内容时产生的笔记，希望也能对您有所帮助。值得注意的是，作者作为初学者，表述难免有误或不全面，望多批评指正。 如有任何问题，欢迎您随时与我联系：Hauyu.Chen@Gmail.com
版权声明：本文由 Hov 所有，发布于 http://hov.space/ ，转载请注明出处。

0 前言 上一篇提到了监督学习的概念，本文要讲的线性回归就属于监督学习。 本文通过单变量线性回归讲述线性回归思想。值得注意的是，特征x并非只能是一个，也可以有x1、x2、x3···，这就是多特征的问题了（后续会提到）。事实上，单特征和多特征的思想是一样的，本文讨论单特征（只有一个特征x）的情形。

1 要点 1.1 单变量线性回归模型 上述公式（线性函数）为一个最简单的线性回归模型，hθ(x)为我们的假设函数，x是训练集中给的数据，θ0、θ1为未知参数，我们需要计算出合适的θ0、θ1的值。
1.2 Cost Function（代价函数） 上述公式为代价函数的定义，hθ(x)是预测值，x(i)是训练集中第i组数据中的特征，y(i)是训练集中第i组数据中的结果，J(θ0,θ1)表示的是预测值与实际值的误差（方差），误差当然越小越好，所以我们的目标就是最小化Cost Function，即找出合适的θ0、 θ1使得J(θ0,θ1)最小，这样说明数据拟合得最好。

2 思路 我们来引入一个场景，我们想实现房价的预测。
 房价取决于多方面的因素：面积、地段、楼层等等。为方便讨论，我们先不考虑多变量的情况，只考虑单变量。也就是在地段、楼层等因素一致的情况下，面积(x)对房价(y)的影响。 房价预测问题其实就是找出面积x和房价y的关系 hθ(x) = θ0 + θ1*x，即根据面积(x)去预测房价(y)。 所以问题的核心是找出合适的θ0、θ1，使得我们的预测 hθ(x) = θ0 + θ1*x 是合理的。 衡量 θ0、θ1 是否合适的标准就是代价函数J(θ0,θ1)，θ0、θ1应使得J(θ0,θ1)尽可能小。  假定我们已经有了一个训练集，里面包含面积x和对应的房价y。以横轴表示面积x，以竖轴表示房价y，根据训练集可绘制图形如下：
注：图示并非真实数据，只作参考。
线性回归要做的就是通过训练，找出面积x与房价y之间对应的关系（线性函数），通过训练，我们可以得出一条表示 hθ(x) 的直线，这就是我们的预测。

3 结语 现在我们应该知道线性回归的思想了，就是通过训练集去计算出假设函数，通过假设函数可以实现对结果的预测。 假设函数最关键的就是找出未知参数 θ0、θ1 ，因为这两个未知参数决定我们预测是否准确。 未知参数 θ0、θ1 的选择通过代价函数 J(θ0,θ1) 来评定，我们要让 J(θ0,θ1) 尽可能小。 那我们怎么计算 θ0、θ1 的值呢？ 下篇将引入“梯度下降”的方法，通过此方法可更快地计算出 θ0、θ1 的值。</description>
    </item>
    
  </channel>
</rss>